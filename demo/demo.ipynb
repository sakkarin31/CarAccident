{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc25fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sakka\\Documents\\Years3\\venv310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train sequences: torch.Size([14016, 24, 9]), Test sequences: torch.Size([3505, 24, 9])\n",
      "üöÄ Starting LSTM Training...\n",
      "‚úÖ Model saved at epoch 1 with Test Loss: 0.000611\n",
      "‚úÖ Model saved at epoch 2 with Test Loss: 0.000611\n",
      "‚úÖ Model saved at epoch 3 with Test Loss: 0.000607\n",
      "‚úÖ Model saved at epoch 4 with Test Loss: 0.000605\n",
      "‚úÖ Model saved at epoch 6 with Test Loss: 0.000605\n",
      "‚úÖ Model saved at epoch 7 with Test Loss: 0.000605\n",
      "‚úÖ Model saved at epoch 8 with Test Loss: 0.000605\n",
      "‚úÖ Model saved at epoch 9 with Test Loss: 0.000604\n",
      "Epoch [10/100] Train Loss: 0.001569, Test Loss: 0.000604, MAE: 0.0086, R2: -0.0910\n",
      "‚úÖ Model saved at epoch 10 with Test Loss: 0.000604\n",
      "‚úÖ Model saved at epoch 11 with Test Loss: 0.000602\n",
      "‚úÖ Model saved at epoch 14 with Test Loss: 0.000602\n",
      "‚úÖ Model saved at epoch 15 with Test Loss: 0.000600\n",
      "Epoch [20/100] Train Loss: 0.001566, Test Loss: 0.000601, MAE: 0.0081, R2: -0.0840\n",
      "‚úÖ Model saved at epoch 21 with Test Loss: 0.000600\n",
      "Epoch [30/100] Train Loss: 0.001566, Test Loss: 0.000604, MAE: 0.0083, R2: -0.1033\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================\n",
    "# 1. Load & Preprocess Data\n",
    "# ==============================\n",
    "df = pd.read_csv('acc_data_final_with_daynight.csv')\n",
    "\n",
    "# üîß ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ: ‡πÉ‡∏ä‡πâ dayfirst=True + format='mixed' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö DD/MM/YYYY\n",
    "df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'], dayfirst=True, format='mixed')\n",
    "df = df.sort_values('datetime').reset_index(drop=True)  # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤\n",
    "\n",
    "# Feature Engineering\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['‡∏Å‡∏•‡∏≤‡∏á‡∏ß‡∏±‡∏ô‡∏Å‡∏•‡∏≤‡∏á‡∏Ñ‡∏∑‡∏ô'] = df['‡∏Å‡∏•‡∏≤‡∏á‡∏ß‡∏±‡∏ô‡∏Å‡∏•‡∏≤‡∏á‡∏Ñ‡∏∑‡∏ô'].map({'‡∏Å‡∏•‡∏≤‡∏á‡∏ß‡∏±‡∏ô': 1, '‡∏Å‡∏•‡∏≤‡∏á‡∏Ñ‡∏∑‡∏ô': 0})\n",
    "\n",
    "# One-hot encode condition\n",
    "df = pd.get_dummies(df, columns=['condition'], prefix='cond', drop_first=True)\n",
    "\n",
    "# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å features ‡πÅ‡∏•‡∏∞ targets\n",
    "feature_cols = [\n",
    "    'temperature_F', 'humidity_%', 'pressure_in',\n",
    "    'hour', 'day_of_week', 'month', '‡∏Å‡∏•‡∏≤‡∏á‡∏ß‡∏±‡∏ô‡∏Å‡∏•‡∏≤‡∏á‡∏Ñ‡∏∑‡∏ô'\n",
    "] + [col for col in df.columns if col.startswith('cond_')]\n",
    "\n",
    "target_cols = ['‡πÄ‡∏Å‡∏¥‡∏î‡πÄ‡∏´‡∏ï‡∏∏', '‡∏£‡∏ñ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤4‡∏•‡πâ‡∏≠acc', '‡∏£‡∏ñ4‡∏•‡πâ‡∏≠acc', '‡∏£‡∏ñ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤4‡∏•‡πâ‡∏≠acc']\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df[target_cols].values\n",
    "\n",
    "# Normalize features\n",
    "scaler_X = MinMaxScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "# ==============================\n",
    "# 2. Create Sequences (Sliding Window)\n",
    "# ==============================\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_length):\n",
    "        X_seq.append(X[i:i + seq_length])\n",
    "        y_seq.append(y[i + seq_length])  # predict next step\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "SEQ_LENGTH = 24  # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á 24 ‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤\n",
    "\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, SEQ_LENGTH)\n",
    "\n",
    "# ‡πÅ‡∏ö‡πà‡∏á train/test (‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏ß‡∏•‡∏≤)\n",
    "split_idx = int(len(X_seq) * 0.8)\n",
    "X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
    "y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"‚úÖ Train sequences: {X_train.shape}, Test sequences: {X_test.shape}\")\n",
    "\n",
    "# ==============================\n",
    "# 3. Define LSTM Model\n",
    "# ==============================\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # ‡πÉ‡∏ä‡πâ output ‡∏Ç‡∏≠‡∏á timestep ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "INPUT_SIZE = X_train.shape[2]\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_SIZE = len(target_cols)\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMModel(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, OUTPUT_SIZE).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# ==============================\n",
    "# 4. TensorBoard Setup\n",
    "# ==============================\n",
    "log_dir = \"runs/lstm_acc_prediction\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# ==============================\n",
    "# 5. Training Loop\n",
    "# ==============================\n",
    "print(\"Starting LSTM Training\")\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_targets.append(batch_y.cpu().numpy())\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "    # Inverse scaling ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì MAE/R2 ‡∏ö‡∏ô‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_targets = np.vstack(all_targets)\n",
    "\n",
    "    all_preds_original = scaler_y.inverse_transform(all_preds)\n",
    "    all_targets_original = scaler_y.inverse_transform(all_targets)\n",
    "\n",
    "    mae = mean_absolute_error(all_targets_original, all_preds_original)\n",
    "    r2 = r2_score(all_targets_original, all_preds_original, multioutput='uniform_average')\n",
    "\n",
    "    # Log to TensorBoard\n",
    "    writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Test', avg_test_loss, epoch)\n",
    "    writer.add_scalar('MAE/Test', mae, epoch)\n",
    "    writer.add_scalar('R2/Test', r2, epoch)\n",
    "\n",
    "    # Log per-target MAE\n",
    "    for i, target_name in enumerate(target_cols):\n",
    "        mae_target = mean_absolute_error(all_targets_original[:, i], all_preds_original[:, i])\n",
    "        writer.add_scalar(f'MAE_per_target/{target_name}', mae_target, epoch)\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] \"\n",
    "              f\"Train Loss: {avg_train_loss:.6f}, Test Loss: {avg_test_loss:.6f}, MAE: {mae:.4f}, R2: {r2:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if avg_test_loss < best_test_loss:\n",
    "        best_test_loss = avg_test_loss\n",
    "        torch.save(model.state_dict(), 'lstm_best_model.pth')\n",
    "        print(f\"‚úÖ Model saved at epoch {epoch+1} with Test Loss: {avg_test_loss:.6f}\")\n",
    "\n",
    "writer.close()\n",
    "print(\"Training Complete!\")\n",
    "\n",
    "# ==============================\n",
    "# 6. Final Evaluation & Save Info\n",
    "# ==============================\n",
    "model.load_state_dict(torch.load('lstm_best_model.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        all_preds.append(outputs.cpu().numpy())\n",
    "        all_targets.append(batch_y.cpu().numpy())\n",
    "\n",
    "all_preds = np.vstack(all_preds)\n",
    "all_targets = np.vstack(all_targets)\n",
    "\n",
    "all_preds_original = scaler_y.inverse_transform(all_preds)\n",
    "all_targets_original = scaler_y.inverse_transform(all_targets)\n",
    "\n",
    "final_mae = mean_absolute_error(all_targets_original, all_preds_original)\n",
    "final_r2 = r2_score(all_targets_original, all_preds_original, multioutput='uniform_average')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL LSTM MODEL PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"MAE: {final_mae:.4f}\")\n",
    "print(f\"R¬≤: {final_r2:.4f}\")\n",
    "\n",
    "# Save to text file\n",
    "with open('lstm_model_info.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"LSTM Model Performance\\n\")\n",
    "    f.write(\"=\"*30 + \"\\n\")\n",
    "    f.write(f\"MAE: {final_mae:.4f}\\n\")\n",
    "    f.write(f\"R¬≤: {final_r2:.4f}\\n\")\n",
    "    f.write(f\"Sequence Length: {SEQ_LENGTH}\\n\")\n",
    "    f.write(f\"Hidden Size: {HIDDEN_SIZE}\\n\")\n",
    "    f.write(f\"Epochs: {NUM_EPOCHS}\\n\")\n",
    "\n",
    "print(\"‚úÖ Saved model info to 'lstm_model_info.txt'\")\n",
    "print(\"‚úÖ Model saved as 'lstm_best_model.pth'\")\n",
    "print(\"‚úÖ TensorBoard logs saved in 'runs/lstm_acc_prediction'\")\n",
    "\n",
    "# ==============================\n",
    "# 7. View TensorBoard\n",
    "# ==============================\n",
    "print(\"\\nTo view training logs in TensorBoard, run:\")\n",
    "print(\"   tensorboard --logdir=runs/lstm_acc_prediction\")\n",
    "print(\"   then open http://localhost:6006\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
